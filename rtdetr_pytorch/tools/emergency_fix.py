#!/usr/bin/env python
"""
Final Emergency Fix for RT-DETR Face Detection
This script fixes the Hungarian matcher batch size issue
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

import yaml
import torch
from pathlib import Path

def create_final_emergency_config():
    """Create the final working emergency config"""
    
    # Create the emergency config directory
    config_dir = Path("configs/rtdetr/emergency_final")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    # Create main config with all fixes
    main_config = {
        'task': 'detection',
        'num_classes': 1,
        'num_landmarks': 5,
        
        # Output directory
        'output_dir': './output/emergency_final',
        
        # Training settings
        'epoches': 10,
        'clip_max_norm': 0.1,
        'log_step': 5,
        'checkpoint_step': 2,
        
        # Runtime settings
        'sync_bn': False,
        'find_unused_parameters': False,
        'use_amp': False,
        'use_ema': False,
        
        # Model configuration
        'model': 'RTDETR',
        'criterion': 'SetCriterion',
        'postprocessor': 'RTDETRPostProcessor',
        
        # RTDETR configuration
        'RTDETR': {
            'backbone': 'PResNet',
            'encoder': 'HybridEncoder',
            'decoder': 'RTDETRTransformer',
            'multi_scale': [640]  # Fixed size
        },
        
        # Backbone configuration
        'PResNet': {
            'depth': 50,
            'variant': 'd',
            'freeze_at': 0,
            'return_idx': [1, 2, 3],
            'num_stages': 4,
            'freeze_norm': True,
            'pretrained': True
        },
        
        # Encoder configuration
        'HybridEncoder': {
            'in_channels': [512, 1024, 2048],
            'feat_strides': [8, 16, 32],
            'hidden_dim': 256,
            'use_encoder_idx': [2],
            'num_encoder_layers': 1,
            'nhead': 8,
            'dim_feedforward': 1024,
            'dropout': 0.0,
            'enc_act': 'gelu',
            'pe_temperature': 10000,
            'expansion': 1.0,
            'depth_mult': 1,
            'act': 'silu',
            'eval_spatial_size': [640, 640]
        },
        
        # Decoder configuration
        'RTDETRTransformer': {
            'in_channels': [512, 1024, 2048],
            'feat_channels': [512, 1024, 2048],
            'feat_strides': [8, 16, 32],
            'hidden_dim': 256,
            'num_levels': 3,
            'num_queries': 300,
            'num_decoder_layers': 6,
            'num_denoising': 50,
            'eval_idx': -1,
            'eval_spatial_size': [640, 640]
        },
        
        # Focal loss settings
        'use_focal_loss': True,
        
        # Post-processor
        'RTDETRPostProcessor': {
            'num_top_queries': 300
        },
        
        # Loss configuration
        'SetCriterion': {
            'weight_dict': {
                'loss_vfl': 1.0,
                'loss_bbox': 5.0,
                'loss_giou': 2.0
            },
            'losses': ['vfl', 'boxes'],
            'alpha': 0.75,
            'gamma': 2.0,
            'matcher': {
                'type': 'HungarianMatcher',
                'weight_dict': {
                    'cost_class': 2,
                    'cost_bbox': 5,
                    'cost_giou': 2
                },
                'alpha': 0.25,
                'gamma': 2.0
            }
        },
        
        # Optimizer configuration
        'optimizer': {
            'type': 'AdamW',
            'lr': 0.0001,
            'betas': [0.9, 0.999],
            'weight_decay': 0.0001
        },
        
        # Learning rate scheduler
        'lr_scheduler': {
            'type': 'MultiStepLR',
            'milestones': [8],
            'gamma': 0.1
        },
        
        # Dataset configuration - FIXED: Use batch_size=1 to avoid matcher issues
        'train_dataloader': {
            'type': 'DataLoader',
            'dataset': {
                'type': 'FaceLandmarkDataset',
                'img_folder': './dataset/faces/train/images/',
                'ann_file': './dataset/faces/train/annotations.json',
                'num_landmarks': 5,
                'transforms': {
                    'type': 'ComposeFaceLandmark',
                    'ops': [
                        {'type': 'Resize', 'size': [640, 640]},
                        {'type': 'ToImageTensor'},
                        {'type': 'ConvertDtype'},
                        {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}
                    ]
                }
            },
            'shuffle': True,
            'batch_size': 1,  # âœ… FIXED: Use batch_size=1 to avoid matcher issues
            'num_workers': 0,
            'drop_last': False,  # âœ… FIXED: Don't drop last incomplete batch
            'collate_fn': 'default_collate_fn'
        },
        
        'val_dataloader': {
            'type': 'DataLoader',
            'dataset': {
                'type': 'FaceLandmarkDataset',
                'img_folder': './dataset/faces/val/images/',
                'ann_file': './dataset/faces/val/annotations.json',
                'num_landmarks': 5,
                'transforms': {
                    'type': 'ComposeFaceLandmark',
                    'ops': [
                        {'type': 'Resize', 'size': [640, 640]},
                        {'type': 'ToImageTensor'},
                        {'type': 'ConvertDtype'},
                        {'type': 'ConvertBox', 'out_fmt': 'cxcywh', 'normalize': True}
                    ]
                }
            },
            'shuffle': False,
            'batch_size': 1,  # âœ… FIXED: Use batch_size=1
            'num_workers': 0,
            'drop_last': False,
            'collate_fn': 'default_collate_fn'
        }
    }
    
    # Save main config
    main_config_path = config_dir / 'main.yml'
    with open(main_config_path, 'w') as f:
        yaml.dump(main_config, f, default_flow_style=False, sort_keys=False)
    
    return main_config_path

def create_better_dummy_dataset():
    """Create better dummy dataset with consistent targets"""
    
    dataset_dir = Path("dataset/faces")
    
    for split in ['train', 'val']:
        split_dir = dataset_dir / split
        img_dir = split_dir / 'images'
        img_dir.mkdir(parents=True, exist_ok=True)
        
        # Create multiple consistent dummy annotations
        annotations = {}
        
        # Create several dummy samples
        for i in range(5):
            annotations[f'dummy_{i}'] = {
                'filename': f'dummy_{i}.jpg',
                'boxes': [[100 + i*10, 100 + i*10, 200 + i*10, 200 + i*10]],  # xyxy format in pixels
                'landmarks': [[
                    0.2 + i*0.05, 0.2 + i*0.02,   # left eye
                    0.3 + i*0.05, 0.2 + i*0.02,   # right eye  
                    0.25 + i*0.05, 0.25 + i*0.02, # nose
                    0.22 + i*0.05, 0.3 + i*0.02,  # left mouth
                    0.28 + i*0.05, 0.3 + i*0.02   # right mouth
                ]]
            }
        
        ann_file = split_dir / 'annotations.json'
        import json
        with open(ann_file, 'w') as f:
            json.dump(annotations, f, indent=2)
        
        print(f"âœ… Created better dummy dataset: {ann_file} ({len(annotations)} samples)")

def test_final_config(config_path):
    """Test the final emergency config"""
    
    print(f"\nğŸ§ª Testing final config: {config_path}")
    
    try:
        # Test 1: Load config
        from src.core import YAMLConfig
        cfg = YAMLConfig(str(config_path))
        print("âœ… Config loaded successfully")
        
        # Test 2: Create model
        model = cfg.model
        model.eval()
        print("âœ… Model created successfully")
        
        # Test 3: Test forward pass
        dummy_input = torch.randn(1, 3, 640, 640)
        with torch.no_grad():
            outputs = model(dummy_input)
        print("âœ… Forward pass successful")
        print(f"   Outputs: {list(outputs.keys())}")
        
        # Test 4: Test dataset
        dataset = cfg.train_dataloader.dataset
        print(f"âœ… Dataset has {len(dataset)} samples")
        
        img, target = dataset[0]
        print("âœ… Dataset access successful")
        print(f"   Image shape: {img.shape}")
        print(f"   Target keys: {list(target.keys())}")
        print(f"   Boxes shape: {target['boxes'].shape}")
        print(f"   Labels shape: {target['labels'].shape}")
        
        # Test 5: Test single-sample dataloader
        train_loader = cfg.train_dataloader
        for images, targets in train_loader:
            print("âœ… Dataloader successful")
            print(f"   Batch size: {len(targets)}")
            print(f"   Images shape: {images.shape}")
            print(f"   Target 0 boxes: {targets[0]['boxes'].shape}")
            break
        
        # Test 6: Test criterion with single sample
        criterion = cfg.criterion
        with torch.no_grad():
            loss_dict = criterion(outputs, targets)
        print("âœ… Loss computation successful")
        print(f"   Losses: {list(loss_dict.keys())}")
        for k, v in loss_dict.items():
            print(f"   {k}: {v.item():.4f}")
        
        # Test 7: Test training step
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)
        
        # Forward pass
        outputs = model(images, targets)
        loss_dict = criterion(outputs, targets)
        losses = sum(loss_dict[k] * criterion.weight_dict[k] 
                    for k in loss_dict.keys() if k in criterion.weight_dict)
        
        # Backward pass
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()
        
        print("âœ… Training step successful")
        print(f"   Total loss: {losses.item():.4f}")
        
        # Test 8: Test postprocessor
        model.eval()
        with torch.no_grad():
            outputs = model(images)
            orig_sizes = torch.stack([t["orig_size"] for t in targets], dim=0)
            results = cfg.postprocessor(outputs, orig_sizes)
        
        print("âœ… Postprocessor successful")
        print(f"   Results: {len(results)} predictions")
        if len(results) > 0:
            result = results[0]
            print(f"   Result keys: {list(result.keys())}")
            print(f"   Predicted boxes: {result['boxes'].shape}")
            print(f"   Predicted scores: {result['scores'].shape}")
        
        print(f"\nğŸ‰ ALL TESTS PASSED! Final config is working perfectly!")
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_training_script(config_path):
    """Create a simple training script"""
    
    training_script = f'''#!/usr/bin/env python
"""
Simple RT-DETR Face Detection Training Script
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

import torch
from src.core import YAMLConfig
from src.solver import TASKS

def main():
    print("ğŸš€ Starting RT-DETR Face Detection Training")
    
    # Load config
    cfg = YAMLConfig('{config_path}')
    
    # Create solver  
    solver = TASKS[cfg.yaml_cfg['task']](cfg)
    
    # Run training
    print("ğŸ“ Training for", cfg.epoches, "epochs")
    solver.fit()
    
    print("âœ… Training completed!")

if __name__ == '__main__':
    main()
'''
    
    script_path = Path("tools/train_emergency.py")
    with open(script_path, 'w') as f:
        f.write(training_script)
    
    return script_path

def main():
    print("ğŸ¯ RT-DETR Final Emergency Fix")
    print("="*50)
    
    print("\n1ï¸âƒ£ Creating final emergency configuration...")
    config_path = create_final_emergency_config()
    print(f"âœ… Final config created: {config_path}")
    
    print("\n2ï¸âƒ£ Creating better dummy dataset...")
    create_better_dummy_dataset()
    
    print("\n3ï¸âƒ£ Testing final configuration...")
    success = test_final_config(config_path)
    
    if success:
        print(f"\nğŸ¯ SUCCESS! Everything is working perfectly!")
        
        # Create training script
        script_path = create_training_script(config_path)
        print(f"âœ… Created training script: {script_path}")
        
        print(f"\nğŸ“ Ready to train! Choose one of these options:")
        print(f"   1. Quick test:     python tools/train.py -c {config_path} --test-only")
        print(f"   2. Full training:  python {script_path}")
        print(f"   3. Custom:         python tools/train.py -c {config_path}")
        
        print(f"\nğŸ’¡ Next steps for real face detection:")
        print(f"   1. Replace dummy dataset with real face images and annotations")
        print(f"   2. Increase batch size (start with 4, then 8, 16)")
        print(f"   3. Increase epochs (50-100 for real training)")
        print(f"   4. Add data augmentation transforms")
        print(f"   5. Add landmark support if needed")
        
        print(f"\nğŸ—ï¸  To add your own dataset:")
        print(f"   1. Create folder: dataset/faces/train/images/")
        print(f"   2. Put your images in that folder")
        print(f"   3. Create annotations.json with format:")
        print(f"      {{")
        print(f"        'image_id': {{")
        print(f"          'filename': 'image.jpg',")
        print(f"          'boxes': [[x1, y1, x2, y2]],  # pixel coordinates")
        print(f"          'landmarks': [[x1,y1,x2,y2,x3,y3,x4,y4,x5,y5]]  # normalized coordinates") 
        print(f"        }}")
        print(f"      }}")
        
    else:
        print(f"\nâŒ Still having issues. The error details are above.")
        print(f"ğŸ“§ If you need help, please share the full error message.")

if __name__ == '__main__':
    main()