# tools/debug_validation_loss.py
"""
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ validation loss ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á
"""

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

import torch
import numpy as np
from src.core import YAMLConfig
from src.data.coco.coco_utils_fixed import get_coco_api_from_dataset

def debug_validation_issues():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô validation pipeline"""
    
    print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Validation Loss")
    print("=" * 60)
    
    # 1. ‡πÇ‡∏´‡∏•‡∏î config ‡πÅ‡∏•‡∏∞ model
    cfg = YAMLConfig('configs/rtdetr/rtdetr_r50vd_face_landmark.yml')
    model = cfg.model
    criterion = cfg.criterion
    postprocessor = cfg.postprocessor
    
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dataset
    val_dataset = cfg.val_dataloader.dataset
    print(f"üìä Val dataset size: {len(val_dataset)}")
    
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà samples
    for i in range(min(3, len(val_dataset))):
        img, target = val_dataset[i]
        print(f"\nSample {i}:")
        print(f"  Boxes range: [{target['boxes'].min():.3f}, {target['boxes'].max():.3f}]")
        
        if 'landmarks' in target:
            print(f"  Landmarks range: [{target['landmarks'].min():.3f}, {target['landmarks'].max():.3f}]")
            if target['landmarks'].max() > 1.0:
                print("  ‚ùå ERROR: Landmarks ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ normalize!")
                return False
        else:
            print("  ‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏°‡∏µ landmarks!")
            return False
    
    print("‚úÖ Dataset normalization ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
    
    # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model output vs target alignment
    model.eval()
    val_loader = cfg.val_dataloader
    
    with torch.no_grad():
        for images, targets in val_loader:
            outputs = model(images[:1])  # ‡πÅ‡∏Ñ‡πà 1 image
            
            print(f"\nü§ñ Model outputs:")
            for key, value in outputs.items():
                if isinstance(value, torch.Tensor):
                    print(f"  {key}: {value.shape}, range=[{value.min():.3f}, {value.max():.3f}]")
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ landmarks alignment
            if 'pred_landmarks' in outputs:
                pred_lmks = outputs['pred_landmarks'][0, 0, :]  # First query
                target_lmks = targets[0]['landmarks'][0, :]     # First face
                
                print(f"\nüìç Landmarks comparison:")
                print(f"  Predicted: {pred_lmks[:6].tolist()}")  # ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏Ñ‡πà 3 ‡∏à‡∏∏‡∏î‡πÅ‡∏£‡∏Å
                print(f"  Target:    {target_lmks[:6].tolist()}")
                
                diff = torch.abs(pred_lmks - target_lmks).mean()
                print(f"  Mean difference: {diff:.3f}")
                
                if diff > 0.5:
                    print("  ‚ö†Ô∏è  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏™‡∏π‡∏á - ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ coordinate space")
            
            break
    
    # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö loss computation
    print(f"\nüí∞ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö loss computation:")
    
    with torch.no_grad():
        try:
            loss_dict = criterion(outputs, targets[:1])
            
            total_loss = 0
            for loss_name, loss_value in loss_dict.items():
                if loss_name in criterion.weight_dict:
                    weighted_loss = loss_value * criterion.weight_dict[loss_name]
                    total_loss += weighted_loss
                    print(f"  {loss_name}: {loss_value:.4f} (weight: {criterion.weight_dict[loss_name]}) = {weighted_loss:.4f}")
            
            print(f"  Total weighted loss: {total_loss:.4f}")
            
            # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÅ‡∏ï‡πà‡∏•‡∏∞ loss component
            landmark_loss = loss_dict.get('loss_landmarks', 0)
            if landmark_loss > 10:
                print(f"  ‚ùå Landmark loss ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: {landmark_loss:.4f}")
                print(f"     ‡∏•‡∏≠‡∏á‡∏•‡∏î weight ‡∏à‡∏≤‡∏Å {criterion.weight_dict.get('loss_landmarks', 'N/A')} ‡πÄ‡∏õ‡πá‡∏ô 0.5")
            
        except Exception as e:
            print(f"  ‚ùå Error computing loss: {e}")
            return False
    
    # 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö COCO API conversion
    print(f"\nüéØ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö COCO conversion:")
    try:
        coco_gt = get_coco_api_from_dataset(val_dataset)
        print(f"  ‚úÖ COCO GT created: {len(coco_gt.anns)} annotations")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö evaluation
        orig_sizes = torch.stack([t["orig_size"] for t in targets[:1]], dim=0)
        results = postprocessor(outputs, orig_sizes)
        
        if results and len(results[0]['boxes']) > 0:
            print(f"  ‚úÖ Postprocessor output: {len(results[0]['boxes'])} detections")
        else:
            print(f"  ‚ö†Ô∏è  No detections from postprocessor")
            
    except Exception as e:
        print(f"  ‚ùå COCO conversion error: {e}")
        return False
    
    return True


def suggest_fixes():
    """‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç"""
    
    print(f"\nüîß ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:")
    print(f"=" * 60)
    
    print(f"1. üìâ ‡∏•‡∏î landmark loss weight:")
    print(f"   - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å loss_landmarks: 5.0 ‡πÄ‡∏õ‡πá‡∏ô 1.0")
    print(f"   - ‡πÉ‡∏ô config file: weight_dict")
    
    print(f"\n2. üéØ ‡∏õ‡∏£‡∏±‡∏ö learning rate:")
    print(f"   - ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.0001 ‡πÄ‡∏õ‡πá‡∏ô 0.00005")
    print(f"   - ‡πÄ‡∏û‡∏¥‡πà‡∏° warmup epochs")
    
    print(f"\n3. üìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data normalization:")
    print(f"   - ‡πÉ‡∏ä‡πâ SanitizeLandmarks transform")
    print(f"   - clamp landmarks ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô [0,1]")
    
    print(f"\n4. ‚öñÔ∏è  Progressive training:")
    print(f"   - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ detection ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (landmark weight = 0)")
    print(f"   - ‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏û‡∏¥‡πà‡∏° landmark weight")
    
    print(f"\n5. üîç Monitor specific metrics:")
    print(f"   - ‡∏î‡∏π landmark loss ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å")
    print(f"   - ‡∏î‡∏π detection AP")
    print(f"   - ‡∏î‡∏π NME (Normalized Mean Error)")


if __name__ == '__main__':
    success = debug_validation_issues()
    
    if success:
        print(f"\n‚úÖ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")
        suggest_fixes()
    else:
        print(f"\n‚ùå ‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡πà‡∏≠‡∏ô")
        suggest_fixes()